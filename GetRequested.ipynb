{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Handle New Data Requests Automatically\n",
    "- beginning of notebook is assumed to be interactive until the requests have been checked\n",
    "- all progress and exception logging is done only for main loop\n",
    "- copy and paste the e-mail response and send from gcs.cmip6.ldeo@gmail.com account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `gsurl` is the GCS url for the dataset\n",
    "- `ds_dir` is our dataset identifier ( activity_id/institution_id/.../variable_id/grid_label ) NO version included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gcsfs\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import os\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myconfig\n",
    "from mydataset import dir2url_, dir2dict\n",
    "from mytasks import Check, Download, ReadFiles, SaveAsZarr, Upload, Cleanup\n",
    "from mysearch import esgf_search\n",
    "#from myidentify import gsurl2search, gsurl2dsdir\n",
    "from myutilities import search_df, remove_from_GC_bucket\n",
    "from myrequest import requests, set_request_id\n",
    "from myresponse import response, get_details, dict_to_dfcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly connect to Search Status Page for updating\n",
    "# Anyone can view: https://docs.google.com/spreadsheets/d/1yAt7604tVt7OXXZUyL2uALtGP2WVa-Pb5NMuTluFsAc/edit?usp=sharing\n",
    "\n",
    "json_keyfile = '/home/naomi/json/CMIP6-d0cb1df722d1.json'\n",
    "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(json_keyfile, scope)\n",
    "gc = gspread.authorize(credentials)\n",
    "sheet_name = \"CMIP6_UpdateSchedule\"\n",
    "sh = gc.open(sheet_name)\n",
    "wks = sh.worksheet(\"Searches\")\n",
    "\n",
    "# read the first row to get the column labels\n",
    "columns = wks.row_values(3)\n",
    "col_status = columns.index('status')\n",
    "col_run = columns.index('last run')\n",
    "col_drive = columns.index('HD')\n",
    "col_dataset = columns.index('current dataset')\n",
    "col_node = columns.index('search node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE ESGF Search here\n",
    "node_pref = myconfig.node_pref\n",
    "dtype = myconfig.dtype\n",
    "hd = '/h123'\n",
    "myconfig.local_target_prefix = hd + '/naomi/zarr-minimal/'\n",
    "dir2local = dir2url_(myconfig.local_target_prefix)\n",
    "\n",
    "update_ESGF = True\n",
    "update_Needed = True\n",
    "\n",
    "search_node = 'llnl'\n",
    "#search_node = 'dkrz'\n",
    "ESGF_site = dtype[search_node]\n",
    "\n",
    "print('zarrs will be written to: ',myconfig.local_target_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE GCS\n",
    "fs     = gcsfs.GCSFileSystem(token='anon', access='read_only',cache_timeout=-1)\n",
    "df_GCS = pd.read_csv('https://cmip6.storage.googleapis.com/pangeo-cmip6-noQC.csv', dtype='unicode')\n",
    "df_GCS['ds_dir'] = df_GCS.apply(lambda row: myconfig.target_format % row,axis=1)\n",
    "\n",
    "# make available to all modules (except those)\n",
    "myconfig.fs = fs\n",
    "myconfig.df_GCS = df_GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new Google Sheet requests\n",
    "- by default, only the new rows from the sheet are considered\n",
    "- specifying a list of rows or emails will add older entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior = pd.read_csv('csv/requests.csv', dtype='unicode', encoding='latin1')\n",
    "\n",
    "rows = []   \n",
    "emails = []\n",
    "#rows = [4] #range(0,5) #[0,5,9]   # line number - 2\n",
    "#emails = ['c.wang@princeton.edu']\n",
    "\n",
    "df_request_new, dtrouble = requests(df_prior,rows=rows,emails=emails)\n",
    "request_id = set_request_id()\n",
    "\n",
    "# Check for mal-formed requests (non-existent variables, etc)\n",
    "if len(dtrouble)>=1:\n",
    "    print(dtrouble)\n",
    "\n",
    "# print all active requests:\n",
    "display(df_request_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by index\n",
    "#type = 'input'\n",
    "type = 'last'\n",
    "\n",
    "if type == 'input':\n",
    "    resp = input('index?  (after entering number, click on next cell to advance)')\n",
    "    df_request_new = df_request_new.loc[[int(resp)]]\n",
    "else:\n",
    "    timestamps = df_request_new.Timestamp.unique()\n",
    "    #df_request_new = df_request_new[df_request_new.Timestamp == timestamps[-1]]\n",
    "    df_request_new = df_request_new.loc[[3]]\n",
    "\n",
    "df_request_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asearch = {}\n",
    "search_keys = {'table':'table_id','experiments':'experiment_id','variables':'variable_id','models':'source_id','members':'member_id'}\n",
    "\n",
    "for item, row in df_request_new.iterrows():\n",
    "    asearch['table_id'] = [row.table]\n",
    "    request = item\n",
    "    for key in search_keys.keys():\n",
    "        if key == 'table':\n",
    "            continue\n",
    "        klist = row[key]\n",
    "        if not ('All' in klist)|('One' in klist):  # Note, we no longer get just one member_id without specifying which\n",
    "            asearch[search_keys[key]] = klist\n",
    "\n",
    "label = f'request-{request}'\n",
    "\n",
    "try:\n",
    "    search_row = wks.find(label).row\n",
    "    print(search_row)\n",
    "except:\n",
    "    print('need another row in Google Form')\n",
    "    wks.append_row([label])\n",
    "    search_row = wks.find(label).row\n",
    "    \n",
    "date = datetime.now().strftime('%Y-%m-%d')\n",
    "wks.update_cell(search_row, col_run + 1, date)\n",
    "wks.update_cell(search_row, col_drive + 1, hd)\n",
    "wks.update_cell(search_row, col_node + 1, search_node)\n",
    "\n",
    "col_activity   = columns.index('activity_id')\n",
    "col_table      = columns.index('table_id')\n",
    "col_experiment = columns.index('experiment_id')\n",
    "col_variable   = columns.index('variable_id')\n",
    "\n",
    "for key in asearch.keys():\n",
    "    vals = asearch[key]\n",
    "    if key in columns:\n",
    "        col = columns.index(key)\n",
    "        if len(vals) == 1:\n",
    "            wks.update_cell(search_row, col + 1, f'{vals[0]}')\n",
    "        else:\n",
    "            wks.update_cell(search_row, col + 1, f'{vals}')            \n",
    "        \n",
    "label, asearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe of ESGF search results\n",
    "\n",
    "if update_ESGF:\n",
    "    x = [value for key,value in asearch.items()]\n",
    "    searches = [p for p in itertools.product(*x)]\n",
    "\n",
    "    dESGF = []\n",
    "    for s in searches:\n",
    "        dsearch = dict(zip(asearch.keys(),s))\n",
    "        print(dsearch)\n",
    "        df = esgf_search(dsearch, server=ESGF_site)\n",
    "        if len(df)>0:\n",
    "            dESGF += [df]\n",
    "\n",
    "    df_ESGF = pd.concat(dESGF)\n",
    "    df_ESGF.to_csv(f'csv/ESGF_{label}.csv',index=False)\n",
    "else:\n",
    "    df_ESGF = pd.read_csv(f'csv/ESGF_{label}.csv', dtype='unicode')\n",
    "\n",
    "len(df_ESGF), len(df_ESGF.ds_dir.unique())\n",
    "#df_ESGF.ds_dir.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe of all needed\n",
    "\n",
    "if update_Needed:\n",
    "    df_ESGF['cversion'] = [int(s[1:]) for s in df_ESGF.version_id]\n",
    "    df_ESGF = df_ESGF.sort_values(by=['cversion'])\n",
    "    df_ESGF = df_ESGF.drop_duplicates(subset =[\"ds_dir\",\"ncfile\"],keep='last')\n",
    " \n",
    "    df_GCS['ds_dir'] = df_GCS.apply(lambda row: myconfig.target_format % row,axis=1)\n",
    "    df_GCS = df_GCS[df_GCS.version != 'unknown']\n",
    "    df_GCS['cversion'] = [int(s) for s in df_GCS.version]\n",
    "    df_GCS = df_GCS.sort_values(by=['version'])\n",
    "    df_GCS = df_GCS.drop_duplicates(subset =[\"ds_dir\"],keep='last')\n",
    "\n",
    "    df = pd.merge(df_ESGF, df_GCS, how='outer', on=['ds_dir'], indicator=True, suffixes=('', '_y'),)\n",
    "    df_new_dataset = df[df._merge == 'left_only'] \n",
    "    \n",
    "    df_check = df[df._merge == 'both'] # we might want to add these if the ESGF version is newer than the GCS version\n",
    "    # New versions (at least 2 days newer) which exist at ESGF\n",
    "    if len(df_check) > 0:\n",
    "        df_check['dversion'] = df_check.apply(lambda row: row.cversion - row.cversion_y,axis=1)\n",
    "        df_check = df_check.sort_values(by=['dversion'])\n",
    "        df_new_version = df_check[df_check.dversion > 2] # at least 2 days newer\n",
    "\n",
    "        # Want to append together df_new_dataset and df_new_version\n",
    "        keys = ['activity_id', 'institution_id', 'source_id', 'experiment_id', 'member_id', 'table_id', 'variable_id', 'grid_label', 'version_id', 'ncfile', 'file_size', 'url', 'data_node', 'ds_dir', 'node_order', 'start', 'stop']\n",
    "        df_new_version = df_new_version[keys]\n",
    "        df_new_dataset = df_new_dataset[keys]\n",
    "\n",
    "        df_needed = df_new_dataset.append(df_new_version)\n",
    "    else:\n",
    "        df_needed = df_new_dataset\n",
    "    df_needed['version'] = [s[1:] for s in df_needed.version_id]\n",
    "\n",
    "    num_stores = 0\n",
    "    if len(df_needed) > 0:\n",
    "        num_stores = df_needed.ds_dir.nunique() \n",
    "        print(f'needed: nfiles={len(df_needed)}, nstores={num_stores}')\n",
    "    \n",
    "        df_needed['member'] = [int(s.split('r')[-1].split('i')[0]) for s in df_needed['member_id']]\n",
    "        df_needed = df_needed.sort_values(by=['member'])\n",
    "        #df_needed['zsize'] = [df_needed[df_needed.ds_dir==zs]['file_size'].sum() for zs in df_needed['ds_dir']]\n",
    "        #df_needed = df_needed.sort_values(by=['zsize'])\n",
    "        df_needed.to_csv(f'csv/needed_{label}.csv',index=False)\n",
    "else:\n",
    "    df_needed = pd.read_csv(f'csv/needed_{label}.csv', dtype='unicode')\n",
    "\n",
    "print('Variables')\n",
    "try:\n",
    "    for var in df_needed.variable_id.unique():\n",
    "        print(var,df_needed[df_needed.variable_id==var].ds_dir.nunique())\n",
    "\n",
    "    print('\\nExperiments')\n",
    "    for exp in df_needed.experiment_id.unique():\n",
    "        print(exp,df_needed[df_needed.experiment_id==exp].ds_dir.nunique())\n",
    "except:\n",
    "    print('no new data available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make available to all modules\n",
    "myconfig.df_needed = df_needed\n",
    "ds_dirs = df_needed.ds_dir.unique()\n",
    "numdsets = len(ds_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_log  = f'logs/progress_{label}.log'\n",
    "failure_log   = f'logs/failure_{label}.log'\n",
    "success_log   = f'logs/success_{label}.log'\n",
    "logs = {1:progress_log, 2:failure_log, 3:success_log}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ierr:\n",
    "- 0 : proceed with next task\n",
    "- 1 : write to progress_log, go to next dataset (finished or try again)\n",
    "- 2 : write to failure_log,  go to next dataset (mark as un-usable - do not try again until problem is solved) \n",
    "- 3 : write to success_log,  go to next dataset (dataset added to cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(file,str,verbose=True):\n",
    "    date = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "    f = open(file,'a')\n",
    "    if verbose:\n",
    "        print(str)\n",
    "    f.write(f'{date}:{str}\\n')\n",
    "    f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh the gcsfs\n",
    "fs.invalidate_cache()\n",
    "date = str(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "for log in [1,2,3]:   \n",
    "    write_log(logs[log],f'starting at {date}') \n",
    "zdict = {}\n",
    "\n",
    "for item, ds_dir in enumerate(ds_dirs):\n",
    "    #if item > 0:\n",
    "    #    continue\n",
    "    print(f'\\n{item}/{numdsets-1}',ds_dir)\n",
    "\n",
    "    #skip = 'EC-Earth3'\n",
    "    skip = 'none'\n",
    "    if skip in ds_dir:\n",
    "        write_log(progress_log,f'{ds_dir} skipping {skip}') \n",
    "        continue\n",
    "\n",
    "    version = df_needed[df_needed.ds_dir==ds_dir].version.values[0]\n",
    "        \n",
    "    (ierr, exc) = Check(ds_dir, version, dir2local)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir} {exc}'); continue\n",
    "                     \n",
    "    (gfiles, version, ierr, exc) = Download(ds_dir)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "        \n",
    "    search_row = wks.find(label).row\n",
    "    wks.update_cell(search_row, col_dataset + 1, ds_dir)\n",
    "    \n",
    "    (ds,ierr,exc) = ReadFiles(ds_dir, gfiles, version, dir2dict)\n",
    "    \n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "\n",
    "    (version,ierr,exc) = SaveAsZarr(ds_dir, ds, dir2local)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "        \n",
    "    (zbdir, gsurl, ierr,exc) = Upload(ds_dir, version, dir2local)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "\n",
    "    (ierr,exc) = Cleanup(ds_dir, version, gfiles, dir2local)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "\n",
    "    vlist = get_details(ds_dir, ds, dir2local)\n",
    "    zdict[item] = vlist\n",
    "\n",
    "    date = datetime.now().strftime('%H:%M, %b%d')\n",
    "    status_str = f'{item+1} of {numdsets} at {date}'\n",
    "    search_row = wks.find(label).row\n",
    "    wks.update_cell(search_row, col_status + 1, status_str)\n",
    "\n",
    "    write_log(success_log,f'{zbdir} saved to {gsurl}'); continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs://cmip6/CMIP/CCCma/CanESM5/piControl/r1i1p1f1/Omon/mlotst/gn/\n",
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a table of acquired data to send in email to requestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(zdict) == 0 :\n",
    "    print('nothing else to do')\n",
    "    exit\n",
    "else:\n",
    "    dz = dict_to_dfcat(zdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_master_new = pd.concat([df_GCS, dz],sort=True)\n",
    "except:\n",
    "    df_master_new = df_GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldict = []\n",
    "names = \"\"\n",
    "print('Re: CMIP6 GCS Data Request (Responses)')\n",
    "for row in df_request_new.values:\n",
    "    rdict = dict(zip(df_request_new.keys(),row))\n",
    "    #print(rdict)\n",
    "    name = rdict['requester']\n",
    "    timestamp = rdict['Timestamp']\n",
    "    names += name\n",
    "    del rdict['response status']\n",
    "    ldict += [rdict]\n",
    "    dfr = df_request_new[df_request_new.Timestamp == timestamp]\n",
    "    \n",
    "    print('Dear',name+':')\n",
    "    print('\\n  Here are the results from your recent CMIP6 data request(s).  The master catalog, https://cmip6.storage.googleapis.com/pangeo-cmip6.csv, will be updated with the nightly build.')\n",
    "    #if len(dtrouble)>=1:\n",
    "    #    print('\\n '+dtrouble)\n",
    "    print('\\n  Please note: ')\n",
    "    print('      - Data for some models (e.g., CAS/FGOALS-f3-L and NUIST/NESM3) must be obtained directly from servers which are too slow or unresponsive. ')\n",
    "    print('      - We exclude data with known errors (as reported at ES-DOC) from the official listing at https://errata.es-doc.org/ .')\n",
    "    print('        However, data labelled status=resolved or severity=low are included in the master catalog.')\n",
    "    \n",
    "\n",
    "    print('      - Some data we have not been able to clean up enough to get it concatenated and save to zarr. Other datasets are only available for disjointed time periods.')\n",
    "    print('\\n  See the sample Jupyter Notebook at https://gist.github.com/naomi-henderson/ed1801d8ee8b992dda252f8b126876a5 for a quick introduction to accessing the data.')\n",
    "    print('  For general xarray help, see https://xarray-contrib.github.io/xarray-tutorial/')\n",
    "    print('\\nFrom the folks at:\\n  The Climate Data Science Lab\\n  Division of Ocean and Climate Physics\\n  LDEO/Columbia University')\n",
    "    print('\\n--------------------------')\n",
    "\n",
    "    print('\\nrequest:')\n",
    "    display(rdict)\n",
    "\n",
    "    print('\\nresponse:')\n",
    "    #try:\n",
    "    #    print('old stores deleted:\\n',num_removed,'\\n')\n",
    "    #    print('new stores added:\\n',len(dz),'\\n')\n",
    "    #except:\n",
    "    #    print(f'no new data available at ESGF API search node {ESGF_site}')\n",
    "\n",
    "    #print('\\n',dfr,len(df_master_new))\n",
    "    table = response(dfr,df_master_new)\n",
    "\n",
    "    print(\"\\navailable data:\\n  this includes your new stores but does not include datasets marked 'onhold', 'wontfix' or 'new' in the ES-DOC ERRATA\")\n",
    "    display(table)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv csv/request_new.csv csv/requests.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xr.open_zarr(fs.get_mapper(gsurl),consolidated=True)\n",
    "#xr.open_zarr(zbdir,consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo-fall2020",
   "language": "python",
   "name": "pangeo-fall2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
