{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd \n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime, calendar, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutilities import search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    tdir = 's3://cmip6-pds/DCPP/'\n",
    "    acts = fs.ls(tdir)\n",
    "    activity_ids = sorted(list(set([td.split('/')[-1] for td in acts])))\n",
    "    print(activity_ids)    \n",
    "    for activity_id in activity_ids:\n",
    "        aname = activity_id.replace('/','-')\n",
    "        GCfile = f'S3_ours/S3_DCPP-{aname}.txt'\n",
    "        print('making:',GCfile)\n",
    "        path = f's3://cmip6-pds/DCPP/{aname}/**/.zmetadata'\n",
    "        flist = fs.glob(path)\n",
    "        print(activity_id,len(flist))\n",
    "        with open(GCfile, \"w\") as file:\n",
    "            file.write('\\n'.join(flist))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AerChemMIP', 'C4MIP', 'CDRMIP', 'CFMIP', 'CMIP', 'DAMIP', 'FAFMIP', 'GMMIP', 'HighResMIP', 'LS3MIP', 'LUMIP', 'OMIP', 'PAMIP', 'PMIP', 'RFMIP', 'ScenarioMIP']\n"
     ]
    }
   ],
   "source": [
    "tdir = 's3://cmip6-pds/CMIP6/'\n",
    "acts = fs.ls(tdir)\n",
    "activity_ids = sorted(list(set([td.split('/')[-1] for td in acts])))\n",
    "activity_ids.remove('old_catalogs')\n",
    "print(activity_ids)\n",
    "#activity_ids = ['PAMIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making: S3_ours/S3_AerChemMIP.txt\n",
      "AerChemMIP 8879\n",
      "making: S3_ours/S3_C4MIP.txt\n",
      "C4MIP 2992\n",
      "making: S3_ours/S3_CDRMIP.txt\n",
      "CDRMIP 778\n",
      "making: S3_ours/S3_CFMIP.txt\n",
      "CFMIP 2970\n",
      "making: S3_ours/S3_CMIP.txt\n",
      "CMIP 125959\n",
      "making: S3_ours/S3_DAMIP.txt\n",
      "DAMIP 22740\n",
      "making: S3_ours/S3_FAFMIP.txt\n",
      "FAFMIP 420\n",
      "making: S3_ours/S3_GMMIP.txt\n",
      "GMMIP 960\n",
      "making: S3_ours/S3_HighResMIP.txt\n",
      "HighResMIP 1604\n",
      "making: S3_ours/S3_LS3MIP.txt\n",
      "LS3MIP 187\n",
      "making: S3_ours/S3_LUMIP.txt\n",
      "LUMIP 1119\n",
      "making: S3_ours/S3_OMIP.txt\n",
      "OMIP 329\n",
      "making: S3_ours/S3_PAMIP.txt\n",
      "PAMIP 42619\n",
      "making: S3_ours/S3_PMIP.txt\n",
      "PMIP 253\n",
      "making: S3_ours/S3_RFMIP.txt\n",
      "RFMIP 4731\n",
      "making: S3_ours/S3_ScenarioMIP.txt\n",
      "ScenarioMIP 138233\n"
     ]
    }
   ],
   "source": [
    "for activity_id in activity_ids:\n",
    "    aname = activity_id.replace('/','-')\n",
    "    GCfile = f'S3_ours/S3_{aname}.txt'\n",
    "    print('making:',GCfile)\n",
    "    try:\n",
    "        path = f's3://cmip6-pds/CMIP6/{activity_id}/**/.zmetadata'\n",
    "        flist = fs.glob(path)\n",
    "        print(activity_id,len(flist))\n",
    "        with open(GCfile, \"w\") as file:\n",
    "            file.write('\\n'.join(flist))    \n",
    "    except:\n",
    "        print(f'skipping {activity_id}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Make new catalog\n",
    "#    1. Collect listings of all files in GC to make dataframe 'dz_GC'\n",
    "#       b. read in list of zarr stores and turn into df with 8-tuple dataset id\n",
    "\n",
    "S3files = sorted(glob('S3_ours/S3_*.txt'))\n",
    "zmetas = []\n",
    "for S3file in S3files:\n",
    "    with open(S3file, \"r\") as file:\n",
    "        zmetas += file.read().split('\\n')\n",
    "\n",
    "ddict = {}\n",
    "for item, tdir in enumerate(zmetas):\n",
    "    vstore = tdir.split('/.zmetadata')[0].split('cmip6-pds/')[-1]\n",
    "    if 'DCPP' in tdir:\n",
    "        vstore = vstore + '/v00000000'\n",
    "    vlist = vstore.split('/')[-9:]\n",
    "    zstore = 's3://cmip6-pds/'+vstore+'/'\n",
    "    vlist += [zstore]\n",
    "    ddict[item] = vlist\n",
    "    \n",
    "\n",
    "dz = pd.DataFrame.from_dict(ddict, orient='index')\n",
    "dz = dz.rename(columns={0: \"activity_id\", 1: \"institution_id\", 2:\"source_id\",\n",
    "                        3:\"experiment_id\",4:\"member_id\",5:\"table_id\",6:\"variable_id\",\n",
    "                        7:\"grid_label\",8:\"version\",9:\"zstore\"}) #,12:\"size\"})\n",
    "\n",
    "dz[\"dcpp_init_year\"] = dz.member_id.map(lambda x: float(x.split(\"-\")[0][1:] if x.startswith(\"s\") else np.nan))\n",
    "dz[\"member_id\"] = dz[\"member_id\"].map(lambda x: x.split(\"-\")[-1] if x.startswith(\"s\") else x)\n",
    "dz[\"version\"] = dz[\"version\"].map(lambda x: x[1:])\n",
    "dz[\"zstore\"] = dz[\"zstore\"].map(lambda x: x[:-10] if x.startswith(\"s3://cmip6-pds/DCPP\") else x)\n",
    "\n",
    "dz = dz[['activity_id','institution_id','source_id','experiment_id','member_id','table_id','variable_id'\n",
    "         ,'grid_label','zstore','dcpp_init_year','version']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/anaconda3/envs/pangeo-forge/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3156: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('s3://cmip6-pds/CMIP6/DAMIP/NASA-GISS/GISS-E2-1-G/hist-nat/r2i1p1f1/AERmon/bldep/gn/v20180906/',\n",
       " 's3://cmip6-pds/CMIP6/AerChemMIP/AS-RCEC/TaiESM1/histSST/r1i1p1f1/AERmon/od550aer/gn/v20200310/')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine this with the errata info to get the two catalogs\n",
    "# A. Make new noQC catalog\n",
    "#    2. read dz_exclude from errata files\n",
    "\n",
    "dz_exclude = pd.read_csv('csv/errata-files.csv')\n",
    "dz_exclude['zstore'] = ['s3://cmip6-pds/' + s.replace('#','/v').replace('.','/')+'/'  for s in dz_exclude.file_id]\n",
    "dz_exclude.zstore.values[0], dz.zstore.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497701 168186 12986\n"
     ]
    }
   ],
   "source": [
    "# A. Make new noQC catalog\n",
    "#    5. using vstore=zstore+version, update dz_new with status, severity and url columns\n",
    "# this can be made much more efficient - with merge, etc\n",
    "\n",
    "# Find vstores (= zstore+version) in dz_GC which have issues at ES-DOC\n",
    "set_A = set(sorted(list(dz.zstore.values))) \n",
    "set_B = set(sorted(list(dz_exclude.zstore.values)))\n",
    "\n",
    "in_both = sorted(list(set_A.intersection(set_B)))\n",
    "print(len(set_A),len(set_B),len(in_both))\n",
    "\n",
    "status = []\n",
    "severity = []\n",
    "url = []\n",
    "for index, row in dz.iterrows():\n",
    "    zstore = row.zstore\n",
    "    if zstore in in_both:\n",
    "        dze = dz_exclude[dz_exclude.zstore==zstore]\n",
    "        status += [dze.status.values[0]]\n",
    "        severity += [dze.severity.values[0]]\n",
    "        url += [dze.issue_url.values[0]]\n",
    "    else:\n",
    "        status += ['good']\n",
    "        severity += ['none']\n",
    "        url += ['none']\n",
    "\n",
    "dz['status'] = status\n",
    "dz['severity'] = severity\n",
    "dz['issue_url'] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497701"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_s3 = dz.copy()\n",
    "len(dz_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496165"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import myconfig\n",
    "dz_s3['ds_dir'] = dz_s3.apply(lambda row: myconfig.target_format % row,axis=1)\n",
    "\n",
    "dz_DCPP = dz_s3[dz_s3.activity_id=='DCPP']\n",
    "dz_nDCPP = dz_s3[dz_s3.activity_id!='DCPP']\n",
    "\n",
    "dz_nDCPP = dz_nDCPP.sort_values(by=['version'])\n",
    "dz_nDCPP = dz_nDCPP.drop_duplicates(subset =[\"ds_dir\"],keep='last')\n",
    "dz_new = pd.concat([dz_nDCPP,dz_DCPP])\n",
    "len(dz_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496165 491362\n"
     ]
    }
   ],
   "source": [
    "# MAKE SURE NEW CATALOG IS LARGER THAN OLD\n",
    "dz_old = pd.read_csv('csv/s3_pangeo-cmip6-noQC.csv', dtype='unicode')\n",
    "\n",
    "print(len(dz_new),len(dz_old))\n",
    "assert len(dz_new) >= len(dz_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    6b. save new noQC catalog and then upload to GC\n",
    "dz_new = dz_new[['activity_id', 'institution_id', 'source_id', 'experiment_id',\n",
    "       'member_id', 'table_id', 'variable_id', 'grid_label', \n",
    "       'zstore', 'dcpp_init_year', 'version', 'status', 'severity', 'issue_url']]\n",
    "\n",
    "dz_new = dz_new.sort_values(by=['activity_id','source_id','experiment_id','member_id'])\n",
    "dz_new.to_csv('csv/s3_pangeo-cmip6-noQC.csv', mode='w+', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Make new standard catalog\n",
    "#    1. eliminate harmless issues from dz_exclude\n",
    "#       these should all be properly evaluated - I just made a first guess\n",
    "\n",
    "dz_exclude = dz_exclude[dz_exclude.status != 'resolved']\n",
    "dz_exclude = dz_exclude[dz_exclude.severity != 'low']\n",
    "dz_exclude = dz_exclude[dz_exclude.issue_uid != 'b6302400-3620-c8f1-999b-d192c0349084']\n",
    "dz_exclude = dz_exclude[dz_exclude.issue_uid != '45f9e7b9-1844-7a92-8b54-10d954e621db']\n",
    "dz_exclude = dz_exclude[dz_exclude.issue_uid != '4aa40e49-b2d4-0b29-a6b1-c80ee8dce11a']\n",
    "dz_exclude = dz_exclude[dz_exclude.issue_uid != '2f6b5963-f87e-b2df-a5b0-2f12b6b68d32']\n",
    "dz_exclude = dz_exclude[dz_exclude.issue_uid != '61fb170e-91bb-4c64-8f1d-6f5e342ee421']\n",
    "dz_exclude = dz_exclude[dz_exclude.issue_uid != '90cac29b-eaff-c450-8621-ea31e305a40e']\n",
    "dz_exclude = dz_exclude[dz_exclude.issue_uid != '8fbd8df5-c349-315b-9ec3-5a2f4ec4ec63']\n",
    "dz_exclude = dz_exclude[dz_exclude.issue_uid != 'ad5ca671-39d0-39ed-bf4f-6c8fb1a06047']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496165 60528 5345\n"
     ]
    }
   ],
   "source": [
    "# B. Make new standard catalog\n",
    "#    2. use this (smaller) list of issues to eliminate the more serious issues from standard catalog\n",
    "\n",
    "# Find zstores in both:\n",
    "set_A = set(sorted(list(dz_new.zstore.values))) \n",
    "set_B = set(sorted(list(dz_exclude.zstore.values)))\n",
    "\n",
    "in_both = sorted(list(set_A.intersection(set_B)))\n",
    "print(len(set_A),len(set_B),len(in_both))\n",
    "\n",
    "dfz = dz_new.copy()\n",
    "dfz['issue'] = [value in in_both for value in dfz.zstore.values]\n",
    "dz_issues = dfz[dfz.issue]\n",
    "dz_clean  = dfz[dfz.issue==False]\n",
    "dz_orig = pd.concat([dfz, dz_issues, dz_issues]).drop_duplicates(keep=False)\n",
    "dz_orig = dz_orig.drop(['issue'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490820 486021\n"
     ]
    }
   ],
   "source": [
    "# MAKE SURE NEW CATALOG IS LARGER THAN OLD\n",
    "dfcat = pd.read_csv('csv/s3_pangeo-cmip6.csv')\n",
    "print(len(dz_orig),len(dfcat))\n",
    "assert len(dz_orig) >= len(dfcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Make new standard catalog\n",
    "#    3b. save new standard catalog and then upload to GC\n",
    "\n",
    "dz_orig.drop(['status','severity','issue_url'],1).to_csv('csv/s3_pangeo-cmip6.csv', mode='w+', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(datetime.datetime.now().strftime(\"%Y%m%d\"))\n",
    "old_cat_loc = f'old_catalogs_s3/pangeo-cmip6-{date}-noQC.csv'\n",
    "old_cat = f'old_catalogs/pangeo-cmip6-{date}-noQC.csv'\n",
    "os.system(f'cp csv/s3_pangeo-cmip6-noQC.csv {old_cat_loc}')\n",
    "os.system(f'gzip {old_cat_loc}')\n",
    "os.system(f'rm {old_cat_loc}')\n",
    "ret = os.system(f'/usr/local/bin/aws s3 cp {old_cat_loc}.gz s3://cmip6-pds/CMIP6/{old_cat}.gz')\n",
    "\n",
    "old_cat_loc = f'old_catalogs_s3/s3_pangeo-cmip6-{date}.csv'\n",
    "old_cat = f'old_catalogs/s3_pangeo-cmip6-{date}.csv'\n",
    "os.system(f'cp csv/s3_pangeo-cmip6.csv {old_cat_loc}')\n",
    "os.system(f'gzip {old_cat_loc}')\n",
    "ret = os.system(f'/usr/local/bin/aws s3 cp {old_cat_loc}.gz s3://cmip6-pds/CMIP6/{old_cat}.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = os.system('/usr/local/bin/aws s3 cp csv/s3_pangeo-cmip6-noQC.csv s3://cmip6-pds/pangeo-cmip6-noQC.csv')\n",
    "os.system(f'gzip csv/s3_pangeo-cmip6-noQC.csv')\n",
    "ret = os.system('/usr/local/bin/aws s3 cp csv/s3_pangeo-cmip6-noQC.csv.gz s3://cmip6-pds/pangeo-cmip6-noQC.csv.gz')\n",
    "os.system(f'gunzip csv/s3_pangeo-cmip6-noQC.csv.gz')\n",
    "if ret != 0:\n",
    "    print('noQC upload not working')\n",
    "    \n",
    "ret = os.system('/usr/local/bin/aws s3 cp csv/s3_pangeo-cmip6.csv s3://cmip6-pds/pangeo-cmip6.csv')\n",
    "if ret != 0:\n",
    "    print('noQC upload not working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo-forge",
   "language": "python",
   "name": "pangeo-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
