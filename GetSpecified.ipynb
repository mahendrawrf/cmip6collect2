{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gcsfs\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import myconfig\n",
    "from mydataset import id2dict_, dir2url_, dir2dict\n",
    "from mytasks import Check, Download, ReadFiles, SaveAsZarr, Upload, Cleanup\n",
    "from mysearch import esgf_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE ESGF Search here\n",
    "node_pref = myconfig.node_pref\n",
    "dtype = myconfig.dtype\n",
    "myconfig.local_target_prefix = '/h113/naomi/zarr-minimal/'\n",
    "dir2local = dir2url_(myconfig.local_target_prefix)\n",
    "\n",
    "ESGF_site = dtype['llnl']\n",
    "#ESGF_site = dtype['dkrz']\n",
    "\n",
    "print('zarrs will be written to: ',myconfig.local_target_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE GCS\n",
    "fs     = gcsfs.GCSFileSystem(token='anon', access='read_only',cache_timeout=-1)\n",
    "df_GCS = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv', dtype='unicode')\n",
    "\n",
    "# make available to all modules (except those)\n",
    "myconfig.fs = fs\n",
    "myconfig.df_GCS = df_GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_mips = ['CMIP', 'ScenarioMIP', 'DAMIP']\n",
    "\n",
    "fx_tables = ['AERfx', 'Efx', 'IfxAnt', 'IfxGre', 'Ofx', 'fx']\n",
    "yr_tables = ['Eyr', 'IyrAnt', 'IyrGre', 'Oyr']\n",
    "other_tables = ['Odec', 'E1hrClimMon','Oclim'] \n",
    "\n",
    "core_experiments = [\n",
    "    '1pctCO2', 'abrupt-4xCO2',  'historical', 'piControl' \n",
    "    ,'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp534-over', 'ssp585'\n",
    "                   ]\n",
    "more_experiments = [\n",
    "     'piControl-spinup', 'amip-hist', 'esm-hist', 'esm-piControl', 'esm-piControl-spinup'\n",
    "    ,'1pctCO2-bgc','lgm', 'past1000', 'amip'\n",
    "                   ]\n",
    "                   \n",
    "core_Amon_2dvars = ['evspsbl', 'hfls', 'pr', 'prc', 'ps', 'psl', 'sfcWind', 'tas', 'ts', 'uas', 'vas','huss','hurs']\n",
    "flux_Amon_2dvars = ['rlds', 'rlus', 'rsds', 'rsus', 'hfds', 'hfls', 'hfss','tauu','tauv']\n",
    "core_Omon_2dvars = ['tos', 'sos', 'zos']\n",
    "flux_Omon_2dvars = ['tauuo', 'tauvo']\n",
    "\n",
    "core_Amon_3dvars = ['ta', 'ua', 'va', 'zg', 'wap', 'hur', 'hus']\n",
    "core_Omon_3dvars = ['masscello', 'so', 'thetao', 'umo', 'uo', 'vmo', 'vo', 'wmo', 'wo']\n",
    "core_Omon_tracers = ['chl', 'chlos', 'dfe', 'dfeos', 'epc100', 'fgco2', 'intpp', 'no3', 'no3os', 'phyc', 'phycos', 'phydiat', 'phydiatos', 'si', 'sios', 'spco2', 'zooc', 'zoocos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick keyword values to specify your search here. Not specifying a particular keyword means it will find all.\n",
    "search = 'test'\n",
    "all_search = {\n",
    "     'table_id'      : ['SImon']\n",
    "    ,'experiment_id' : ['ssp119', 'ssp126']\n",
    "    ,'variable_id'   : [\"sithick\", \"siconc\", \"sisnthick\", \"sisnconc\"]\n",
    "    #,'member_id'     : ['r1i1p1f1']\n",
    "    #,'source_id'     : ['CESM2-WACCM']\n",
    "}\n",
    "\n",
    "search = 'O2d-1f'\n",
    "\n",
    "# define some common searches:\n",
    "if search == 'A2d-1c':\n",
    "    all_search = {'table_id': ['Amon'], 'experiment_id': core_experiments, 'variable_id': core_Amon_2dvars}\n",
    "if search == 'A2d-1f':\n",
    "    all_search = {'table_id': ['Amon'], 'experiment_id': core_experiments, 'variable_id': flux_Amon_2dvars}\n",
    "if search == 'A3d-1c':\n",
    "    all_search = {'table_id': ['Amon'], 'experiment_id': core_experiments, 'variable_id': core_Amon_3dvars}\n",
    "\n",
    "if search == 'O2d-1f':\n",
    "    all_search = {'table_id': ['Omon'], 'experiment_id': core_experiments, 'variable_id': flux_Omon_2dvars}\n",
    "if search == 'O2d-1c':\n",
    "    all_search = {'table_id': ['Omon'], 'experiment_id': core_experiments, 'variable_id': core_Omon_2dvars}\n",
    "if search == 'O3d-1c':\n",
    "    all_search = {'table_id': ['Omon'], 'experiment_id': core_experiments, 'variable_id': core_Omon_3dvars}\n",
    "if search == 'O3d-1t':\n",
    "    all_search = {'table_id': ['Omon'], 'experiment_id': core_experiments, 'variable_id': core_Omon_tracers}\n",
    "\n",
    "if search == 'other-coremips':\n",
    "    all_search = {'table_id': other_tables, 'activity_id': core_mips}\n",
    "if search == 'fx-coremips':\n",
    "    all_search = {'table_id': fx_tables, 'activity_id': core_mips}\n",
    "if search == 'yr-coremips':\n",
    "    all_search = {'table_id': yr_tables, 'activity_id': core_mips}\n",
    "\n",
    "# check if ANOTHER notebook is doing the same search\n",
    "lock_file = f'logs/{search}.lock'\n",
    "resp = 'n'\n",
    "if os.path.exists(lock_file):\n",
    "    resp = input('clear matching logs? (y/n)')\n",
    "if resp == 'y':\n",
    "    command = f'/bin/rm logs/*{search}*'\n",
    "    print(command)\n",
    "    os.system(command)\n",
    "    \n",
    "f = open(lock_file,'w')\n",
    "date = str(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "f.write(f'started {search} at {date}')\n",
    "f.close()\n",
    "\n",
    "label = search\n",
    "all_search, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ESGF = True\n",
    "if update_ESGF:\n",
    "    x = [value for key,value in all_search.items()]\n",
    "    searches = [p for p in itertools.product(*x)]\n",
    "\n",
    "    dESGF = []\n",
    "    for s in searches:\n",
    "        search = dict(zip(all_search.keys(),s))\n",
    "        print(search)\n",
    "        df = esgf_search(search, server=ESGF_site)\n",
    "        if len(df)>0:\n",
    "            dESGF += [df]\n",
    "\n",
    "    df_ESGF = pd.concat(dESGF)\n",
    "    df_ESGF.to_csv(f'csv/ESGF_{label}.csv',index=False)\n",
    "else:\n",
    "    df_ESGF = pd.read_csv(f'csv/ESGF_{label}.csv', dtype='unicode')\n",
    "\n",
    "len(df_ESGF), len(df_ESGF.ds_dir.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df of all needed\n",
    "NewNeeded = True\n",
    "if NewNeeded:\n",
    "    df = pd.merge(df_ESGF,df_GCS, how='outer', indicator=True)\n",
    "    df_needed = df[df._merge == 'left_only']\n",
    "    \n",
    "    keep_keys = df_ESGF.keys()\n",
    "    all_keys = df.keys()\n",
    "    drop_keys = list(set(all_keys) - set(keep_keys))\n",
    "    df_needed = df_needed.drop(drop_keys,1)\n",
    "\n",
    "    num_stores = 0\n",
    "    if len(df_needed) > 0:\n",
    "        num_stores = df_needed.ds_dir.nunique() \n",
    "        print(f'needed: nfiles={len(df_needed)}, nstores={num_stores}')\n",
    "    else:\n",
    "        print('no new data available')\n",
    "        exit    \n",
    "    \n",
    "    df_needed['member'] = [int(s.split('r')[-1].split('i')[0]) for s in df_needed['member_id']]\n",
    "    df_needed = df_needed.sort_values(by=['member'])\n",
    "    #df_needed['zsize'] = [df_needed[df_needed.ds_dir==zs]['file_size'].sum() for zs in df_needed['ds_dir']]\n",
    "    #df_needed = df_needed.sort_values(by=['zsize'])\n",
    "    \n",
    "    df_needed.to_csv(f'csv/needed_{label}.csv',index=False)\n",
    "else:\n",
    "    df_needed = pd.read_csv(f'csv/needed_{label}.csv')\n",
    "\n",
    "print('Variables')\n",
    "for var in df_needed.variable_id.unique():\n",
    "    print(var,df_needed[df_needed.variable_id==var].ds_dir.nunique())\n",
    "\n",
    "print('\\nExperiments')\n",
    "for exp in df_needed.experiment_id.unique():\n",
    "    print(exp,df_needed[df_needed.experiment_id==exp].ds_dir.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make available to all modules\n",
    "myconfig.df_needed = df_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dirs = df_needed.ds_dir.unique()\n",
    "numdsets = len(ds_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_log  = f'logs/progress_{label}.log'\n",
    "failure_log  = f'logs/failure_{label}.log'\n",
    "success_log  = f'logs/success_{label}.log'\n",
    "logs = {1:progress_log, 2:failure_log, 3:success_log}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ierr:\n",
    "- 0 : proceed with next task\n",
    "- 1 : write to progress_log, go to next dataset (finished or try again)\n",
    "- 2 : write to failure_log,  go to next dataset (mark as un-usable - do not try again until problem is solved) \n",
    "- 3 : write to success_log,  go to next dataset (dataset added to cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(file,str,verbose=True):\n",
    "    f = open(file,'a')\n",
    "    if verbose:\n",
    "        print(str)\n",
    "    f.write(f'{str}\\n')\n",
    "    f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the catalog\n",
    "df_GCS = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv', dtype='unicode')\n",
    "\n",
    "# refresh the gcsfs\n",
    "fs.invalidate_cache()\n",
    "\n",
    "for item, ds_dir in enumerate(ds_dirs):\n",
    "\n",
    "    print(f'\\n{item}/{numdsets-1}',ds_dir)\n",
    "\n",
    "    #skip = 'EC-Earth3'\n",
    "    skip = 'none'\n",
    "    if skip in ds_dir:\n",
    "        write_log(progress_log,f'{ds_dir} skipping {skip}') \n",
    "        continue\n",
    "        \n",
    "    (ierr, exc) = Check(ds_dir, dir2local)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir} {exc}'); continue\n",
    "                     \n",
    "    (gfiles, ierr, exc) = Download(ds_dir)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "        \n",
    "    (ds,ierr,exc) = ReadFiles(ds_dir, gfiles, dir2dict)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "\n",
    "    (ierr,exc) = SaveAsZarr(ds_dir, ds, dir2local)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "        \n",
    "    (ierr,exc) = Upload(ds_dir, dir2local)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "\n",
    "    (ierr,exc) = Cleanup(ds_dir, gfiles, dir2local)\n",
    "    if ierr > 0:\n",
    "        write_log(logs[ierr],f'{ds_dir}, {exc}'); continue\n",
    "\n",
    "    write_log(success_log,f'{dir2local(ds_dir)} saved to GCS'); continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo-fall2020",
   "language": "python",
   "name": "pangeo-fall2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
